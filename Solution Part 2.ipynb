{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,cat_cols,cont_cols,layer_count,output_features,embed_size_list,p=0.5): \n",
    "        '''\n",
    "        args:\n",
    "        cat_cols is the tensor of all categorical values (pre-embedding)\n",
    "        n_cont=number of continuous variables (for batch normalization)\n",
    "        cont_cols is the tensor of all continuous values\n",
    "        input_features - number of parameters of input\n",
    "        layer_count - a tuple of number of nodes of each hidden layer\n",
    "        output_features = number of outputs expected\n",
    "        embed_size_list is list of embedding sizes for the categorical values\n",
    "        p = basically, the % of nodes to be nullified during dropout layer\n",
    "       \n",
    "       Approach: In the constructor, create all the layers (Linear, ReLU, Batch and Dropout) for each hidden layer as per layer_count)\n",
    "       and add them to sequential(). Fwd() will have all the data manipulation and final embedding\n",
    "       \n",
    "        '''\n",
    "        super().__init__()\n",
    "        #self.input_features=input_features\n",
    "        self.output_features=output_features\n",
    "        self.layer_count=layer_count\n",
    "        self.embed_size_list=embed_size_list\n",
    "        #Create embeddings from categorical columns as seen in the test case\n",
    "        self.embeddings=[nn.Embedding(base_dim, target_dim) for base_dim, target_dim in embed_size_list]\n",
    "#         print(self.embeddings)\n",
    "        n_cont=cont_cols.shape[1]\n",
    "        self.batch_norm=nn.BatchNorm1d(n_cont)\n",
    "        self.dropout=nn.Dropout(p=0.5)\n",
    "        #To create the layers we need to start with input sizes\n",
    "        n_cont=cont_cols.shape[1]\n",
    "        #We need to find the total number of columns in the data. \n",
    "        #cat_cols currently only has 4 columns which post embedding will go to 23 \n",
    "        n_in=sum(nf for ni,nf in self.embed_size_list)+n_cont\n",
    "#         print('n_in',n_in)\n",
    "        self.layers=[]\n",
    "        for l in self.layer_count:\n",
    "            self.layers.append(nn.Linear(n_in,l))\n",
    "            self.layers.append(nn.ReLU(inplace=True))\n",
    "            self.layers.append(nn.Dropout(p))\n",
    "            self.layers.append(nn.BatchNorm1d(l))            \n",
    "            n_in=l\n",
    "        self.layers.append(nn.Linear(self.layer_count[-1],self.output_features))\n",
    "#         print(self.layers)\n",
    "        self.final_layers=nn.Sequential(*self.layers)\n",
    "#         print(self.final_layers)\n",
    "        \n",
    "    def forward(self,cat_cols, cont_cols):\n",
    "        '''\n",
    "        1. Create the embedding for cat and create one final input value for the forward path (only one time this gets created)\n",
    "        2. Create the dropout for this input layer\n",
    "        3. Create a batch norm for this layer\\\n",
    "        4. Pass this into the all the layers moving forward (starting with the first linear layer)\n",
    "        '''\n",
    "        #Creating the embedding for categorical columns\n",
    "        embeds=[]\n",
    "        for i,e in enumerate(self.embeddings):\n",
    "#             print('e' ,e)\n",
    "            embeds.append(e(cat_cols[:,i]))\n",
    "        cat_final=torch.cat(embeds,axis=1)\n",
    "#         print(cat_final.shape) #17 columns cos 12 for hours, 2 for AM/PM and 4 for days post embedding\n",
    "#         print(cont_cols.shape) # 6 columns one for each of the 6 features\n",
    "        \n",
    "        #Batch normalize the continuous variables first\n",
    "        cont_cols=self.batch_norm(cont_cols)\n",
    "        cat_final=self.dropout(cat_final)\n",
    "        self.X=torch.cat((cat_final, cont_cols),axis=1)\n",
    "#         print('X shape', self.X.shape)\n",
    "#         print('X dtype',self.X.dtype)\n",
    "#         print('X class',type(self.X))\n",
    "        #Dropout for the complete data set\n",
    "        #self.X=self.dropout(self.X)\n",
    "        self.X=self.final_layers(self.X)\n",
    "        return self.X\n",
    "            \n",
    "\n",
    "        #Creating final data set with cat and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (batch_norm): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (final_layers): Sequential(\n",
       "    (0): Linear(in_features=23, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Dropout(p=0.4, inplace=False)\n",
       "    (7): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_model=torch.load('uber_model.pkl')\n",
    "samp_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the input data\n",
    "#pickup_datetime,fare_amount,fare_class,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count\n",
    "samp_pickup_datetime='2010-04-15 16:00:00'\n",
    "samp_pickup_longitude=-73.9\n",
    "samp_pickup_latitude=40.5\n",
    "samp_dropoff_longitude=-73.92\n",
    "samp_dropoff_latitude=40.52\n",
    "samp_passenger_count=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(df, lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    r = 6371  # average radius of Earth in kilometers\n",
    "       \n",
    "    phi1 = np.radians(df[lat1])\n",
    "    phi2 = np.radians(df[lat2])\n",
    "    \n",
    "    delta_phi = np.radians(df[lat2]-df[lat1])\n",
    "    delta_lambda = np.radians(df[long2]-df[long1])\n",
    "     \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c) # in kilometers\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date time as before\n",
    "dict1={'samp_pickup_datetime':'2010-04-15 16:00:00','samp_pickup_longitude':-73.9,'samp_pickup_latitude':40.5,\n",
    "       'samp_dropoff_longitude':-73.92,'samp_dropoff_latitude':40.52,'samp_passenger_count':2\n",
    "      }\n",
    "samp_df=pd.DataFrame(dict1,columns=dict1.keys(),index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_df['samp_pickup_datetime']=pd.to_datetime(samp_df['samp_pickup_datetime'])\n",
    "samp_df['distance']=haversine_distance(samp_df,'samp_pickup_latitude', 'samp_pickup_longitude', 'samp_dropoff_latitude', 'samp_dropoff_longitude')\n",
    "samp_df['hours']=samp_df['samp_pickup_datetime'].dt.hour\n",
    "samp_df['AM_PM']=np.where(samp_df['hours']<12,\"AM\",\"PM\")\n",
    "samp_df['weekday']=samp_df['samp_pickup_datetime'].dt.strftime('%a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats=['hours','weekday','AM_PM']\n",
    "for cat in cats:\n",
    "    samp_df[cat]=samp_df[cat].astype('category')\n",
    "cat_cols=torch.tensor(np.stack([samp_df[cat].cat.codes for cat in cats],axis=1),dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts=['samp_pickup_longitude',\n",
    "       'samp_pickup_latitude', 'samp_dropoff_longitude', 'samp_dropoff_latitude',\n",
    "       'samp_passenger_count', 'distance']\n",
    "cont_cols=torch.tensor(np.stack([samp_df[col].values for col in conts],axis=1),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted fare is 17.3 \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y=samp_model.forward(cat_cols,cont_cols)\n",
    "print('Predicted fare is {:4.4g} '.format(y.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
